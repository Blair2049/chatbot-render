import os
from lightrag import LightRAG, QueryParam
import json
import time
import numpy as np
import tiktoken
from datetime import datetime
import asyncio
import logging

from lightrag.utils import EmbeddingFunc
from lightrag.llm import openai_complete_if_cache, openai_embedding

#########
# Uncomment the below two lines if running in a jupyter notebook to handle the async nature of rag.insert()
# import nest_asyncio
# nest_asyncio.apply()
#########
from dotenv import load_dotenv

# åŠ è½½ .env æ–‡ä»¶
load_dotenv()

# è®¾ç½®å·¥ä½œç›®å½•
WORKING_DIR = "./stakeholder_management_rag_sync"
if not os.path.exists(WORKING_DIR):
    os.makedirs(WORKING_DIR)

# Woodsæ–‡æ¡£è·¯å¾„
WOODS_DIR = "/Users/blairzhang/Desktop/MyProject/LightRAG-main/LightRAG/woods"

# æˆæœ¬ä¼°ç®—é…ç½®
COST_CONFIG = {
    "gpt-4o-mini": {
        "input_cost_per_1k_tokens": 0.00015,  # $0.00015 per 1K input tokens
        "output_cost_per_1k_tokens": 0.0006,  # $0.0006 per 1K output tokens
    },
    "text-embedding-ada-002": {
        "cost_per_1k_tokens": 0.0001,  # $0.0001 per 1K tokens
    }
}

# è¯„åˆ†ç³»ç»Ÿé…ç½®
SCORING_CONFIG = {
    "comprehensiveness_weight": 0.4,
    "diversity_weight": 0.3,
    "empowerment_weight": 0.3,
    "max_score": 10.0
}

class StakeholderManagementChatbot:
    def __init__(self):
        """åˆå§‹åŒ–Stakeholder Management Chatbot"""
        self.rag = None
        self.token_encoder = tiktoken.encoding_for_model("gpt-4o-mini")
        self.cost_stats = {
            "total_input_tokens": 0,
            "total_output_tokens": 0,
            "total_embedding_tokens": 0,
            "total_cost": 0.0,
            "cache_hits": 0,
            "cache_misses": 0
        }
        self.query_history = []
        
    def initialize_rag(self):
        """åˆå§‹åŒ–LightRAGç³»ç»Ÿ - ä½¿ç”¨åŒæ­¥æ–¹æ³•"""
        print("ğŸš€ åˆå§‹åŒ–Stakeholder Management Chatbot...")
        
        # å®šä¹‰LLMå’Œembeddingå‡½æ•°
        async def llm_model_func(
            prompt, system_prompt=None, history_messages=[], keyword_extraction=False, **kwargs
        ) -> str:
            return await openai_complete_if_cache(
                "gpt-4o-mini",
                prompt,
                system_prompt=system_prompt,
                history_messages=history_messages,
                api_key=os.getenv("OPENAI_API_KEY"),
                **kwargs
            )

        async def embedding_func(texts: list[str]) -> np.ndarray:
            return await openai_embedding(
                texts,
                model="text-embedding-ada-002",
                api_key=os.getenv("OPENAI_API_KEY")
            )

        # åˆå§‹åŒ–LightRAGï¼Œä½¿ç”¨READMEä¸­çš„åŒæ­¥é…ç½®
        self.rag = LightRAG(
            working_dir=WORKING_DIR,
            llm_model_func=llm_model_func,
            embedding_func=EmbeddingFunc(
                embedding_dim=1536,
                max_token_size=8192,
                func=embedding_func,
            ),
            # ä½¿ç”¨READMEä¸­çš„addon_paramsé…ç½®
            addon_params={
                "insert_batch_size": 4,
                "language": "Simplified Chinese",
                "entity_types": ["organization", "person", "geo", "event", "project"],
                "example_number": 3
            },
            # å¯ç”¨ç¼“å­˜é…ç½®
            enable_llm_cache=True,
            enable_llm_cache_for_entity_extract=True
        )
        
        print("âœ… LightRAGç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")

    def load_woods_documents(self):
        """è¯»å–Woodsæ–‡æ¡£"""
        documents = []
        woods_files = []
        
        # è·å–æ‰€æœ‰wood_part*.txtæ–‡ä»¶
        for i in range(1, 12):  # 1åˆ°11
            filename = f"wood_part{i}.txt"
            filepath = os.path.join(WOODS_DIR, filename)
            if os.path.exists(filepath):
                woods_files.append(filepath)
        
        # æŒ‰æ–‡ä»¶åæ’åº
        woods_files.sort()
        
        print(f"ğŸ“ æ‰¾åˆ° {len(woods_files)} ä¸ªWoodsideé¡¹ç›®æ–‡æ¡£:")
        for filepath in woods_files:
            filename = os.path.basename(filepath)
            print(f"   - {filename}")
        
        # è¯»å–æ¯ä¸ªæ–‡æ¡£
        for filepath in woods_files:
            try:
                with open(filepath, "r", encoding="utf-8") as f:
                    content = f.read()
                    # æ¸…ç†å†…å®¹ï¼Œç§»é™¤å¯èƒ½å¯¼è‡´APIé”™è¯¯çš„å­—ç¬¦
                    content = content.replace('\x00', '').replace('\ufffd', '')
                    documents.append(content)
                    print(f"âœ… å·²è¯»å–: {os.path.basename(filepath)} ({len(content)} å­—ç¬¦)")
            except Exception as e:
                print(f"âŒ è¯»å–æ–‡ä»¶å¤±è´¥ {filepath}: {e}")
        
        return documents

    def insert_documents(self):
        """æ’å…¥æ–‡æ¡£åˆ°RAGç³»ç»Ÿ - ä½¿ç”¨åŒæ­¥æ–¹æ³•ï¼Œå‚è€ƒREADME"""
        print("\nğŸ“š æ­£åœ¨æ’å…¥Woodsideé¡¹ç›®æ–‡æ¡£åˆ°RAGç³»ç»Ÿ...")
        documents = self.load_woods_documents()
        
        if not documents:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ–‡æ¡£æ–‡ä»¶ï¼")
            return False
        
        try:
            # ä½¿ç”¨READMEä¸­çš„åŒæ­¥æ’å…¥æ–¹æ³•
            print(f"æ­£åœ¨æ’å…¥ {len(documents)} ä¸ªæ–‡æ¡£...")
            
            # æ–¹æ³•1ï¼šé€ä¸ªæ’å…¥ï¼ˆå‚è€ƒREADMEçš„Incremental Insertï¼‰
            for i, doc in enumerate(documents, 1):
                print(f"æ­£åœ¨æ’å…¥æ–‡æ¡£ {i}/{len(documents)}...")
                try:
                    # ä½¿ç”¨åŒæ­¥æ’å…¥æ–¹æ³•
                    self.rag.insert(doc)
                    print(f"âœ… æ–‡æ¡£ {i} æ’å…¥å®Œæˆ")
                except Exception as e:
                    print(f"âš ï¸  æ–‡æ¡£ {i} æ’å…¥å¤±è´¥: {e}")
                    continue
            
            print(f"âœ… æˆåŠŸæ’å…¥ {len(documents)} ä¸ªæ–‡æ¡£åˆ°RAGç³»ç»Ÿ")
            return True
            
        except Exception as e:
            print(f"âŒ æ’å…¥æ–‡æ¡£å¤±è´¥: {e}")
            return False

    def calculate_tokens(self, text):
        """è®¡ç®—æ–‡æœ¬çš„tokenæ•°é‡"""
        return len(self.token_encoder.encode(text))

    def calculate_cost(self, input_tokens, output_tokens, embedding_tokens=0):
        """è®¡ç®—APIè°ƒç”¨æˆæœ¬"""
        # è®¡ç®—LLMæˆæœ¬
        llm_input_cost = (input_tokens / 1000) * COST_CONFIG["gpt-4o-mini"]["input_cost_per_1k_tokens"]
        llm_output_cost = (output_tokens / 1000) * COST_CONFIG["gpt-4o-mini"]["output_cost_per_1k_tokens"]
        
        # è®¡ç®—embeddingæˆæœ¬
        embedding_cost = (embedding_tokens / 1000) * COST_CONFIG["text-embedding-ada-002"]["cost_per_1k_tokens"]
        
        total_cost = llm_input_cost + llm_output_cost + embedding_cost
        
        return {
            "llm_input_cost": llm_input_cost,
            "llm_output_cost": llm_output_cost,
            "embedding_cost": embedding_cost,
            "total_cost": total_cost
        }

    def score_response(self, query, response, mode):
        """è¯„åˆ†ç³»ç»Ÿï¼šåŸºäºREADMEä¸­çš„è¯„ä¼°æ ‡å‡†"""
        scores = {
            "comprehensiveness": 0.0,
            "diversity": 0.0,
            "empowerment": 0.0
        }
        
        # æ£€æµ‹é€šç”¨é—®é¢˜ç±»å‹
        general_questions = [
            "hi", "hello", "hey", "ä½ å¥½", "æ‚¨å¥½",
            "who are you", "what are you", "ä½ æ˜¯è°", "ä½ æ˜¯ä»€ä¹ˆ",
            "how are you", "ä½ å¥½å—", "ä½ å¥½å—ï¼Ÿ",
            "thanks", "thank you", "è°¢è°¢", "è°¢è°¢æ‚¨",
            "bye", "goodbye", "å†è§", "æ‹œæ‹œ"
        ]
        
        query_lower = query.lower().strip()
        is_general_question = any(gq in query_lower for gq in general_questions)
        
        # è®¡ç®—comprehensivenessï¼ˆå®Œæ•´æ€§ï¼‰
        response_length = len(response)
        query_complexity = len(query.split())
        
        if is_general_question:
            # å¯¹äºé€šç”¨é—®é¢˜ï¼Œåªè¦ä¸æ˜¯"Insufficient Data"å°±ç»™é«˜åˆ†
            if "ä¿¡æ¯ä¸è¶³" not in response and "Insufficient Data" not in response:
                scores["comprehensiveness"] = 8.0
            else:
                # å¦‚æœæ˜¯é€šç”¨é—®é¢˜ä½†è¿”å›äº†Insufficient Dataï¼Œç»™è¾ƒä½åˆ†
                scores["comprehensiveness"] = 3.0
        else:
            # å¯¹äºé¡¹ç›®ç›¸å…³é—®é¢˜ï¼Œä½¿ç”¨åŸæœ‰é€»è¾‘
            if response_length > 100 and "ä¿¡æ¯ä¸è¶³" not in response and "Insufficient Data" not in response:
                scores["comprehensiveness"] = min(10.0, response_length / 50)
            else:
                scores["comprehensiveness"] = max(1.0, response_length / 20)
        
        # è®¡ç®—diversityï¼ˆå¤šæ ·æ€§ï¼‰
        unique_words = len(set(response.lower().split()))
        total_words = len(response.split())
        if total_words > 0:
            diversity_ratio = unique_words / total_words
            scores["diversity"] = min(10.0, diversity_ratio * 15)
        
        # è®¡ç®—empowermentï¼ˆå¯å‘æ€§ï¼‰
        empowerment_keywords = ["å»ºè®®", "æ¨è", "è€ƒè™‘", "åˆ†æ", "è¯„ä¼°", "å»ºè®®", "recommend", "consider", "analyze", "evaluate"]
        empowerment_count = sum(1 for keyword in empowerment_keywords if keyword.lower() in response.lower())
        scores["empowerment"] = min(10.0, empowerment_count * 2)
        
        # å¯¹äºé€šç”¨é—®é¢˜ï¼Œå¢åŠ empowermentåˆ†æ•°
        if is_general_question and "ä¿¡æ¯ä¸è¶³" not in response and "Insufficient Data" not in response:
            scores["empowerment"] = min(10.0, scores["empowerment"] + 3.0)
        
        # æ ¹æ®æŸ¥è¯¢æ¨¡å¼è°ƒæ•´åˆ†æ•°
        mode_bonus = {
            "mix": 1.2,
            "hybrid": 1.1,
            "global": 1.0,
            "local": 0.9,
            "naive": 0.8
        }
        
        for key in scores:
            scores[key] *= mode_bonus.get(mode, 1.0)
            scores[key] = min(10.0, scores[key])
        
        # è®¡ç®—åŠ æƒæ€»åˆ†
        total_score = (
            scores["comprehensiveness"] * SCORING_CONFIG["comprehensiveness_weight"] +
            scores["diversity"] * SCORING_CONFIG["diversity_weight"] +
            scores["empowerment"] * SCORING_CONFIG["empowerment_weight"]
        )
        
        return {
            "scores": scores,
            "total_score": round(total_score, 2),
            "mode": mode
        }

    def query_with_analysis(self, question, mode="mix"):
        """å¸¦åˆ†æçš„æŸ¥è¯¢åŠŸèƒ½ - ä½¿ç”¨åŒæ­¥æ–¹æ³•"""
        print(f"\nğŸ” ä½¿ç”¨ {mode} æ¨¡å¼æŸ¥è¯¢: {question}")
        
        start_time = time.time()
        
        try:
            # æ‰§è¡ŒæŸ¥è¯¢ - ä½¿ç”¨åŒæ­¥æ–¹æ³•ï¼Œå¢åŠ top_kå‚æ•°
            result = self.rag.query(question, param=QueryParam(mode=mode, top_k=10))
            
            # è®¡ç®—tokenå’Œæˆæœ¬
            input_tokens = self.calculate_tokens(question)
            output_tokens = self.calculate_tokens(result)
            
            cost_info = self.calculate_cost(input_tokens, output_tokens)
            
            # æ›´æ–°æˆæœ¬ç»Ÿè®¡
            self.cost_stats["total_input_tokens"] += input_tokens
            self.cost_stats["total_output_tokens"] += output_tokens
            self.cost_stats["total_cost"] += cost_info["total_cost"]
            
            # è¯„åˆ†
            score_info = self.score_response(question, result, mode)
            
            # è®°å½•æŸ¥è¯¢å†å²
            query_record = {
                "timestamp": datetime.now().isoformat(),
                "question": question,
                "mode": mode,
                "response": result,
                "scores": score_info,
                "cost": cost_info,
                "response_time": time.time() - start_time
            }
            self.query_history.append(query_record)
            
            return {
                "response": result,
                "scores": score_info,
                "cost": cost_info,
                "response_time": time.time() - start_time
            }
            
        except Exception as e:
            print(f"âŒ æŸ¥è¯¢å‡ºé”™: {e}")
            return None

    def display_cost_stats(self):
        """æ˜¾ç¤ºæˆæœ¬ç»Ÿè®¡"""
        print("\nğŸ’° æˆæœ¬ç»Ÿè®¡:")
        print(f"   æ€»è¾“å…¥tokens: {self.cost_stats['total_input_tokens']:,}")
        print(f"   æ€»è¾“å‡ºtokens: {self.cost_stats['total_output_tokens']:,}")
        print(f"   æ€»æˆæœ¬: ${self.cost_stats['total_cost']:.4f}")
        print(f"   ç¼“å­˜å‘½ä¸­: {self.cost_stats['cache_hits']}")
        print(f"   ç¼“å­˜æœªå‘½ä¸­: {self.cost_stats['cache_misses']}")

    def display_query_history(self):
        """æ˜¾ç¤ºæŸ¥è¯¢å†å²"""
        print(f"\nğŸ“Š æŸ¥è¯¢å†å² (å…±{len(self.query_history)}æ¡):")
        for i, record in enumerate(self.query_history[-5:], 1):  # æ˜¾ç¤ºæœ€è¿‘5æ¡
            print(f"   {i}. [{record['mode']}] {record['question'][:50]}...")
            print(f"      è¯„åˆ†: {record['scores']['total_score']}/10, æˆæœ¬: ${record['cost']['total_cost']:.4f}")

    def test_different_modes(self):
        """æµ‹è¯•ä¸åŒæŸ¥è¯¢æ¨¡å¼ - ä½¿ç”¨åŒæ­¥æ–¹æ³•"""
        test_questions = [
            "What is the Scarborough gas project?",
            "Who are the key stakeholders in this project?",
            "What are the environmental impacts of the project?",
            "How does the project benefit the local community?",
            "What are the main risks and challenges?"
        ]
        
        modes = ["naive", "local", "global", "hybrid", "mix"]
        
        print(f"\nğŸ§ª æµ‹è¯•ä¸åŒæŸ¥è¯¢æ¨¡å¼:")
        print("=" * 80)
        
        for question in test_questions:
            print(f"\nğŸ“ æµ‹è¯•é—®é¢˜: {question}")
            print("-" * 60)
            
            best_result = None
            best_score = 0
            
            # æ”¶é›†æ‰€æœ‰æ¨¡å¼çš„ç»“æœ
            mode_results = {}
            
            for mode in modes:
                result = self.query_with_analysis(question, mode)
                if result:
                    score = result["scores"]["total_score"]
                    mode_results[mode] = result
                    print(f"   {mode:>8}: è¯„åˆ† {score:>5.1f}/10, æˆæœ¬ ${result['cost']['total_cost']:.4f}")
                    
                    if score > best_score:
                        best_score = score
                        best_result = result
            
            # è¾“å‡ºæœ€ä½³æ¨¡å¼çš„å®Œæ•´ç­”æ¡ˆ
            if best_result:
                print(f"\nğŸ† æœ€ä½³ç»“æœ ({best_result['scores']['mode']} æ¨¡å¼):")
                print(f"   è¯„åˆ†: {best_result['scores']['total_score']}/10")
                print(f"   æˆæœ¬: ${best_result['cost']['total_cost']:.4f}")
                print(f"   å“åº”æ—¶é—´: {best_result['response_time']:.2f}ç§’")
                print(f"\nğŸ“ å®Œæ•´ç­”æ¡ˆ:")
                print(f"   {best_result['response']}")
                print(f"\nğŸ“Š è¯¦ç»†è¯„åˆ†:")
                print(f"   å®Œæ•´æ€§ (Comprehensiveness): {best_result['scores']['scores']['comprehensiveness']:.1f}/10")
                print(f"   å¤šæ ·æ€§ (Diversity): {best_result['scores']['scores']['diversity']:.1f}/10")
                print(f"   å¯å‘æ€§ (Empowerment): {best_result['scores']['scores']['empowerment']:.1f}/10")
            else:
                print("âŒ æ‰€æœ‰æ¨¡å¼éƒ½æŸ¥è¯¢å¤±è´¥")
            
            print("\n" + "="*80)

    def interactive_chat(self):
        """äº¤äº’å¼èŠå¤© - ä½¿ç”¨åŒæ­¥æ–¹æ³•"""
        print("\nğŸ’¬ å¼€å§‹äº¤äº’å¼èŠå¤© (è¾“å…¥ 'quit' é€€å‡º, 'stats' æŸ¥çœ‹ç»Ÿè®¡, 'history' æŸ¥çœ‹å†å²):")
        print("ğŸ“š ä½ å¯ä»¥è¯¢é—®å…³äºWoodside Scarboroughé¡¹ç›®çš„ä»»ä½•é—®é¢˜ï¼")
        print("ğŸ’¡ å»ºè®®é—®é¢˜:")
        print("   - What is the Scarborough gas project?")
        print("   - Who are the key stakeholders?")
        print("   - What are the environmental impacts?")
        print("   - How does the project benefit the community?")
        print("   - What are the main risks and challenges?")
        print("\nğŸ”§ æŸ¥è¯¢æ¨¡å¼é€‰é¡¹:")
        print("   - ç›´æ¥è¾“å…¥é—®é¢˜: è‡ªåŠ¨é€‰æ‹©æœ€ä½³æ¨¡å¼ (æ¨è)")
        print("   - è¾“å…¥ 'mix' + é—®é¢˜: å¼ºåˆ¶ä½¿ç”¨mixæ¨¡å¼")
        print("   - è¾“å…¥ 'hybrid' + é—®é¢˜: å¼ºåˆ¶ä½¿ç”¨hybridæ¨¡å¼")
        print("   - è¾“å…¥ 'global' + é—®é¢˜: å¼ºåˆ¶ä½¿ç”¨globalæ¨¡å¼")
        print("   - è¾“å…¥ 'local' + é—®é¢˜: å¼ºåˆ¶ä½¿ç”¨localæ¨¡å¼")
        print("   - è¾“å…¥ 'naive' + é—®é¢˜: å¼ºåˆ¶ä½¿ç”¨naiveæ¨¡å¼")
        
        while True:
            try:
                user_input = input("\nğŸ‘¤ ä½ : ").strip()
                
                if user_input.lower() in ['quit', 'exit', 'é€€å‡º', 'q']:
                    print("ğŸ‘‹ å†è§!")
                    break
                
                if user_input.lower() == 'stats':
                    self.display_cost_stats()
                    continue
                
                if user_input.lower() == 'history':
                    self.display_query_history()
                    continue
                
                if not user_input:
                    print("ğŸ’¡ è¯·è¾“å…¥æ‚¨çš„é—®é¢˜...")
                    continue
                
                # è§£æç”¨æˆ·è¾“å…¥ï¼Œåˆ¤æ–­æ˜¯å¦æŒ‡å®šäº†æ¨¡å¼
                mode = "best"  # é»˜è®¤è‡ªåŠ¨é€‰æ‹©æœ€ä½³æ¨¡å¼
                question = user_input
                
                # æ£€æŸ¥æ˜¯å¦æŒ‡å®šäº†æ¨¡å¼
                if user_input.lower().startswith("mix "):
                    mode = "mix"
                    question = user_input[4:].strip()
                elif user_input.lower().startswith("hybrid "):
                    mode = "hybrid"
                    question = user_input[7:].strip()
                elif user_input.lower().startswith("global "):
                    mode = "global"
                    question = user_input[7:].strip()
                elif user_input.lower().startswith("local "):
                    mode = "local"
                    question = user_input[6:].strip()
                elif user_input.lower().startswith("naive "):
                    mode = "naive"
                    question = user_input[6:].strip()
                
                if mode == "best":
                    # è‡ªåŠ¨é€‰æ‹©æœ€ä½³æ¨¡å¼
                    print(f"\nğŸ” æ­£åœ¨æµ‹è¯•æ‰€æœ‰æ¨¡å¼ä¸ºé—®é¢˜é€‰æ‹©æœ€ä½³ç­”æ¡ˆ...")
                    modes_to_test = ["naive", "local", "global", "hybrid", "mix"]
                    best_result = None
                    best_score = 0
                    best_mode = "mix"
                    
                    for test_mode in modes_to_test:
                        result = self.query_with_analysis(question, test_mode)
                        if result:
                            score = result["scores"]["total_score"]
                            print(f"   {test_mode:>8}: è¯„åˆ† {score:>5.1f}/10")
                            if score > best_score:
                                best_score = score
                                best_result = result
                                best_mode = test_mode
                    
                    if best_result:
                        print(f"\nğŸ† é€‰æ‹© {best_mode} æ¨¡å¼ (è¯„åˆ†æœ€é«˜: {best_score:.1f}/10)")
                        print(f"ğŸ“ å›ç­”: {best_result['response']}")
                        print(f"ğŸ“Š è¯„åˆ†: {best_result['scores']['total_score']}/10")
                        print(f"ğŸ’° æˆæœ¬: ${best_result['cost']['total_cost']:.4f}")
                        print(f"â±ï¸  å“åº”æ—¶é—´: {best_result['response_time']:.2f}ç§’")
                    else:
                        print("âŒ æ‰€æœ‰æ¨¡å¼éƒ½æŸ¥è¯¢å¤±è´¥")
                else:
                    # ä½¿ç”¨æŒ‡å®šæ¨¡å¼
                    result = self.query_with_analysis(question, mode)
                    if result:
                        print(f"\nğŸ“ å›ç­”: {result['response']}")
                        print(f"ğŸ“Š è¯„åˆ†: {result['scores']['total_score']}/10")
                        print(f"ğŸ’° æˆæœ¬: ${result['cost']['total_cost']:.4f}")
                        print(f"â±ï¸  å“åº”æ—¶é—´: {result['response_time']:.2f}ç§’")
                
            except KeyboardInterrupt:
                print("\nğŸ‘‹ å†è§!")
                break
            except Exception as e:
                print(f"âŒ èŠå¤©å‡ºé”™: {e}")
                print("ğŸ’¡ è¯·å°è¯•é‡æ–°è¾“å…¥æ‚¨çš„é—®é¢˜...")

def main():
    """ä¸»å‡½æ•° - ä½¿ç”¨åŒæ­¥æ–¹æ³•"""
    try:
        # æ£€æŸ¥APIå¯†é’¥
        if not os.getenv("OPENAI_API_KEY"):
            print("âŒ é”™è¯¯: è¯·è®¾ç½®OPENAI_API_KEYç¯å¢ƒå˜é‡")
            print("ğŸ’¡ æç¤º: åœ¨ç»ˆç«¯ä¸­è¿è¡Œ: export OPENAI_API_KEY='your-api-key-here'")
            return
        
        # åˆ›å»ºchatbotå®ä¾‹
        chatbot = StakeholderManagementChatbot()
        
        # åˆå§‹åŒ–RAGç³»ç»Ÿ
        chatbot.initialize_rag()
        
        # æ’å…¥æ–‡æ¡£
        success = chatbot.insert_documents()
        if not success:
            return
        
        # æµ‹è¯•ä¸åŒæ¨¡å¼
        chatbot.test_different_modes()
        
        # æ˜¾ç¤ºåˆå§‹ç»Ÿè®¡
        chatbot.display_cost_stats()
        
        # äº¤äº’å¼èŠå¤©
        chatbot.interactive_chat()
        
        # æœ€ç»ˆç»Ÿè®¡
        print("\n" + "="*60)
        print("ğŸ“ˆ æœ€ç»ˆç»Ÿè®¡æŠ¥å‘Š")
        print("="*60)
        chatbot.display_cost_stats()
        chatbot.display_query_history()
        
    except Exception as e:
        print(f"An error occurred: {e}")

if __name__ == "__main__":
    main() 